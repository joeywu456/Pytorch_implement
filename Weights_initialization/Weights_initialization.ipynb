{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8299bc63",
   "metadata": {},
   "source": [
    "# PyTorch Initialization Methods\n",
    "\n",
    "在這個 Notebook 裡，我們會示範常見的 PyTorch 參數初始化方法與如何使用它們。所有這些方法都在 `torch.nn.init` 模組中提供，且在初始化參數時會自動進入 `no_grad()` 模式，也就是不會被 Autograd 追蹤梯度。\n",
    "\n",
    "## Outline\n",
    "1. `calculate_gain` 取得對應非線性函式的建議增益值（gain）\n",
    "2. 常見填值初始化：\n",
    "   - `uniform_` (均勻分布)\n",
    "   - `normal_` (常態分布)\n",
    "   - `constant_` (常數填充)\n",
    "   - `ones_` / `zeros_` (填充 1 或 0)\n",
    "   - `eye_` (單位矩陣)\n",
    "   - `dirac_` (Dirac delta)\n",
    "3. Xavier/Glorot 初始化：\n",
    "   - `xavier_uniform_`\n",
    "   - `xavier_normal_`\n",
    "4. Kaiming/He 初始化：\n",
    "   - `kaiming_uniform_`\n",
    "   - `kaiming_normal_`\n",
    "5. 其他初始化：\n",
    "   - `trunc_normal_` (截斷常態分布)\n",
    "   - `orthogonal_` (正交初始化)\n",
    "   - `sparse_` (稀疏矩陣初始化)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49111b",
   "metadata": {},
   "source": [
    "## 1. `calculate_gain`\n",
    "\n",
    "PyTorch 提供了一個方便的函式 `calculate_gain(nonlinearity, param=None)` 來取得對應非線性函式的建議增益值 (gain)，常搭配 Xavier/He 初始化時使用。\n",
    "\n",
    "| nonlinearity   | gain (常見設定)                |\n",
    "|----------------|--------------------------------|\n",
    "| linear/identity| 1                              |\n",
    "| conv           | 1                              |\n",
    "| sigmoid        | 1                              |\n",
    "| tanh           | $\\frac{5}{3}$                 |\n",
    "| relu           | $\\sqrt{2}$                    |\n",
    "| leaky_relu     | $\\sqrt{2/(1+\\alpha^2)}$       |\n",
    "| selu           | 3\\~4 (實務可能改用 `linear`) |\n",
    "\n",
    "舉例來說，在使用 `leaky_relu` 時，可將 `param` 設為 negative_slope，用來計算正確的增益值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153a2fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain for ReLU: 1.4142135623730951\n",
      "Gain for Leaky ReLU with negative_slope=0.2: 1.3867504905630728\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "gain_relu = nn.init.calculate_gain('relu')\n",
    "gain_leaky = nn.init.calculate_gain('leaky_relu', param=0.2)\n",
    "\n",
    "print('Gain for ReLU:', gain_relu)\n",
    "print('Gain for Leaky ReLU with negative_slope=0.2:', gain_leaky)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7653fc",
   "metadata": {},
   "source": [
    "## 2. 常見填值初始化\n",
    "\n",
    "以下展示幾種最簡單也最常用的初始化方式：\n",
    "\n",
    "### 2.1 `uniform_`\n",
    "- 以均勻分布 `U(a, b)` 填充 Tensor\n",
    "<img src='./images/Uniform.png' >\n",
    "\n",
    "### 2.2 `normal_`\n",
    "- 以常態分布 `N(mean, std^2)` 填充 Tensor\n",
    "<img src='./images/Normal.png' width=\"600\" >\n",
    "\n",
    "### 2.3 `constant_`\n",
    "- 以固定常數 `val` 填充 Tensor\n",
    "\n",
    "### 2.4 `ones_` / `zeros_`\n",
    "- 分別以 1 或 0 填充 Tensor\n",
    "\n",
    "### 2.5 `eye_`\n",
    "- 只適用於 **2D** Tensor，填充成單位矩陣\n",
    "\n",
    "### 2.6 `dirac_`\n",
    "- 一般用於卷積核，保留盡可能多的輸入通道特徵（相當於\"維持\" identity）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e7016b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform distribution [0,1):\n",
      " tensor([[0.2671, 0.4670, 0.3027, 0.2787, 0.2288],\n",
      "        [0.5017, 0.9847, 0.0516, 0.2456, 0.4050],\n",
      "        [0.1216, 0.8835, 0.2338, 0.3601, 0.3058]])\n",
      "\n",
      "Normal distribution mean=0, std=1:\n",
      " tensor([[-0.0028,  0.2040, -0.1246, -1.0559, -1.4040],\n",
      "        [ 0.1987,  1.4315,  1.4492, -0.3885, -0.4568],\n",
      "        [ 0.8681,  1.0814,  0.8029, -0.1495, -0.1098]])\n",
      "\n",
      "Constant = 0.3:\n",
      " tensor([[0.3000, 0.3000, 0.3000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.3000, 0.3000, 0.3000],\n",
      "        [0.3000, 0.3000, 0.3000, 0.3000, 0.3000]])\n",
      "\n",
      "Ones:\n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "Zeros:\n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "Identity (eye):\n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 建立幾個空的 Tensor 來示範不同初始化\n",
    "w_uniform = torch.empty(3, 5)\n",
    "w_normal = torch.empty(3, 5)\n",
    "w_const = torch.empty(3, 5)\n",
    "w_ones = torch.empty(3, 5)\n",
    "w_zeros = torch.empty(3, 5)\n",
    "w_eye = torch.empty(3, 3)\n",
    "\n",
    "# 初始化示範\n",
    "nn.init.uniform_(w_uniform, a=0.0, b=1.0)\n",
    "nn.init.normal_(w_normal, mean=0.0, std=1.0)\n",
    "nn.init.constant_(w_const, 0.3)\n",
    "nn.init.ones_(w_ones)\n",
    "nn.init.zeros_(w_zeros)\n",
    "nn.init.eye_(w_eye)\n",
    "\n",
    "print('Uniform distribution [0,1):\\n', w_uniform)\n",
    "print('\\nNormal distribution mean=0, std=1:\\n', w_normal)\n",
    "print('\\nConstant = 0.3:\\n', w_const)\n",
    "print('\\nOnes:\\n', w_ones)\n",
    "print('\\nZeros:\\n', w_zeros)\n",
    "print('\\nIdentity (eye):\\n', w_eye)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe2184",
   "metadata": {},
   "source": [
    "## 3. Xavier / Glorot 初始化\n",
    "\n",
    "這兩種方法常見於深度網路的初始權重設置，以避免梯度在前向/反向傳播中出現大量的增大或縮小。\n",
    "\n",
    "- `xavier_uniform_`: 依照\n",
    "  $\n",
    "  U(-a, a)\\quad \\text{where}\\quad a = \\text{gain} \\times \\sqrt{\\frac{6}{fan\\_in + fan\\_out}}\n",
    "  $\n",
    "\n",
    "- `xavier_normal_`: 依照\n",
    "  $\n",
    "  N(0, \\sigma^2)\\quad \\text{where}\\quad \\sigma = \\text{gain} \\times \\sqrt{\\frac{2}{fan\\_in + fan\\_out}}\n",
    "  $\n",
    "\n",
    "其中：\n",
    "- `fan_in`：輸入特徵的數量（輸入層神經元數）\n",
    "- `fan_out`：輸出特徵的數量（輸出層神經元數）\n",
    "- `gain`：根據非線性函數計算得到的增益值\n",
    "\n",
    "這種初始化方法特別適合使用 sigmoid 或 tanh 等激活函數的網路層。\n",
    "\n",
    "**注意**：PyTorch 的 `fan_in`, `fan_out` 是以權重矩陣在\"轉置\"的情況下計算。若你的權重維度是 `[fan_in, fan_out]`，請傳入 `w.T` 來取得正確的初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc56ebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xavier Uniform with ReLU gain:\n",
      " tensor([[-1.0062, -0.5872, -0.1699, -0.4298,  0.1111],\n",
      "        [-0.8025, -0.6520,  0.5045,  0.7045, -0.2590],\n",
      "        [ 0.6635, -0.4752,  0.8562, -0.8945, -0.4691]])\n",
      "\n",
      "Xavier Normal with ReLU gain:\n",
      " tensor([[ 0.4047,  0.5327, -0.2970,  0.1878, -0.0796],\n",
      "        [ 0.4851, -0.5323,  0.2778, -0.7407, -0.4531],\n",
      "        [ 0.3065,  0.2383,  0.7162,  0.9930,  0.1819]])\n"
     ]
    }
   ],
   "source": [
    "# 建立一個維度為 (3, 5) 的 Tensor，對應 linear layer: fan_in=5, fan_out=3\n",
    "w_xavier_uniform = torch.empty(3, 5)\n",
    "w_xavier_normal = torch.empty(3, 5)\n",
    "\n",
    "# 計算適用 ReLU 的 gain\n",
    "gain_relu = nn.init.calculate_gain('relu')\n",
    "\n",
    "# 使用 Xavier 初始化\n",
    "nn.init.xavier_uniform_(w_xavier_uniform, gain=gain_relu) #Attention 是用這種方式初始化\n",
    "nn.init.xavier_normal_(w_xavier_normal, gain=gain_relu)\n",
    "\n",
    "print('Xavier Uniform with ReLU gain:\\n', w_xavier_uniform)\n",
    "print('\\nXavier Normal with ReLU gain:\\n', w_xavier_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0fb2b",
   "metadata": {},
   "source": [
    "## 4. Kaiming / He 初始化\n",
    "\n",
    "Kaiming 初始化（也稱 He 初始化）常用於 ReLU、Leaky ReLU 等激活函式，能在深度網路初始訓練時維持較穩定的梯度。\n",
    "\n",
    "- `kaiming_uniform_`: 依照\n",
    "  $\n",
    "  U(-bound, bound)\\quad \\text{where}\\quad bound = \\text{gain} \\times \\sqrt{\\frac{3}{fan\\_mode}}\n",
    "  $\n",
    "  \n",
    "- `kaiming_normal_`: 依照\n",
    "  $\n",
    "  N(0, \\sigma^2)\\quad \\text{where}\\quad \\sigma = \\text{gain} \\times \\sqrt{\\frac{1}{fan\\_mode}}\n",
    "  $\n",
    "  \n",
    "其中 `fan_mode` 可能是 `fan_in` 或 `fan_out`，對應到是否要在前向傳播或反向傳播中維持方差。默認為 `fan_in`。\n",
    "\n",
    "**注意**：同樣需要注意 Tensor 維度的轉置問題。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af7334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaiming Uniform (fan_in, a=0.2):\n",
      " tensor([[-0.3712,  0.9724,  0.1284, -0.1359, -1.0139],\n",
      "        [ 1.0590, -0.0249,  0.2663, -0.5897,  1.0453],\n",
      "        [-0.9065,  0.1635,  0.2030, -0.4129, -0.3626]])\n",
      "\n",
      "Kaiming Normal (fan_in, a=0.2):\n",
      " tensor([[ 0.2862,  0.5285,  0.6730, -0.2073,  0.0544],\n",
      "        [-0.0753,  0.6546,  0.2149, -0.3965, -0.5786],\n",
      "        [ 0.6175, -0.6493,  1.1136, -0.0701,  0.3327]])\n"
     ]
    }
   ],
   "source": [
    "# 建立一個維度為 (3, 5) 的 Tensor，對應 linear layer: fan_in=5, fan_out=3\n",
    "w_kaiming_uniform = torch.empty(3, 5)\n",
    "w_kaiming_normal = torch.empty(3, 5)\n",
    "\n",
    "# 以 leaky_relu 為例，給定 a=0.2\n",
    "nn.init.kaiming_uniform_(w_kaiming_uniform, a=0.2, mode='fan_in', nonlinearity='leaky_relu')\n",
    "nn.init.kaiming_normal_(w_kaiming_normal, a=0.2, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "print('Kaiming Uniform (fan_in, a=0.2):\\n', w_kaiming_uniform)\n",
    "print('\\nKaiming Normal (fan_in, a=0.2):\\n', w_kaiming_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89a47f",
   "metadata": {},
   "source": [
    "## 5. 其他初始化\n",
    "\n",
    "### 5.1 `trunc_normal_`\n",
    "- 從常態分布截取至 \\([a, b]\\) 範圍內，超過範圍的部分重新抽樣，適合一些比較需要限制初始範圍的情況。\n",
    "\n",
    "### 5.2 `orthogonal_`\n",
    "- 將最後兩個維度展平後，生成一個近似正交矩陣再填入其中。\n",
    "\n",
    "### 5.3 `sparse_`\n",
    "- 生成一個稀疏矩陣（只有少部分元素非 0），非 0 元素從常態分布取樣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357bf787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trunc Normal (a=-2, b=2):\n",
      " tensor([[ 0.4219,  1.2257, -0.2164,  0.3166, -0.2780],\n",
      "        [ 0.0491, -1.7142, -0.3582, -0.7454,  0.2659],\n",
      "        [-0.4679,  0.1626, -0.0602,  0.7535,  0.9937]])\n",
      "\n",
      "Orthogonal (5x5):\n",
      " tensor([[-0.3369, -0.7080, -0.2599,  0.5154,  0.2281],\n",
      "        [ 0.5478,  0.3734, -0.4644,  0.5632,  0.1660],\n",
      "        [ 0.1015, -0.2155, -0.7819, -0.5540, -0.1582],\n",
      "        [ 0.6933, -0.4781,  0.3014, -0.2231,  0.3876],\n",
      "        [ 0.3090, -0.2905,  0.1206,  0.2459, -0.8632]])\n",
      "\n",
      "Sparse (3x5, 80% zero in each column):\n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# trunc_normal_\n",
    "w_trunc_normal = torch.empty(3, 5)\n",
    "nn.init.trunc_normal_(w_trunc_normal, mean=0.0, std=1.0, a=-2.0, b=2.0)\n",
    "\n",
    "# orthogonal_\n",
    "w_orthogonal = torch.empty(5, 5)\n",
    "nn.init.orthogonal_(w_orthogonal, gain=1.0)\n",
    "\n",
    "# sparse_\n",
    "w_sparse = torch.empty(3, 5)\n",
    "nn.init.sparse_(w_sparse, sparsity=0.8, std=0.01)\n",
    "\n",
    "print('Trunc Normal (a=-2, b=2):\\n', w_trunc_normal)\n",
    "print('\\nOrthogonal (5x5):\\n', w_orthogonal)\n",
    "print('\\nSparse (3x5, 80% zero in each column):\\n', w_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54de003",
   "metadata": {},
   "source": [
    "## 結論\n",
    "以上是 PyTorch 初始化函式的一些整理與示範。實務上較常用的初始化方式包括：\n",
    "- **Xavier / Glorot**：對於 sigmoid、tanh 等激活常有效\n",
    "- **Kaiming / He**：對於 ReLU、Leaky ReLU 效果良好\n",
    "\n",
    "也可視需求自行組合 `calculate_gain` 來計算合適的 gain。\n",
    "\n",
    "若對某些模組或層有特別的初始化需求，也可以透過手動的方式在定義 `nn.Module` 後呼叫相應的初始化函式或自訂函式。\n",
    "\n",
    "---\n",
    "### 參考\n",
    "- [PyTorch 官方文件: torch.nn.init](https://pytorch.org/docs/stable/nn.init.html)\n",
    "- He et al., [*Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification* (2015)](https://arxiv.org/abs/1502.01852)\n",
    "- Glorot & Bengio, [*Understanding the difficulty of training deep feedforward neural networks* (2010)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
