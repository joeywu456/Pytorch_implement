{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2285f3eb",
   "metadata": {},
   "source": [
    "# ViT程式碼1-timm庫 實作ViT\n",
    "\n",
    "**reference**\n",
    "\n",
    "- rwightman resource：https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b96d23",
   "metadata": {},
   "source": [
    "**timm 庫**\n",
    "\n",
    "PyTorch Image Models（簡稱 timm）是一個龐大的 PyTorch 代碼集合，包含以下內容：\n",
    "\n",
    "- 影像模型\n",
    "- 層（layers）\n",
    "- 工具（utilities）\n",
    "- 優化器（optimizers）\n",
    "- 調度器（schedulers）\n",
    "- 資料加載器 / 增強（data-loaders / augmentations）\n",
    "- 訓練 / 驗證腳本\n",
    "其目標是將各種最新的 SOTA 模型整合在一起，並具備重現 ImageNet 訓練結果的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607d8e5",
   "metadata": {},
   "source": [
    "**ViT Network architecture**\n",
    "\n",
    "<img src='./images/ViT.png' width=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ab67a",
   "metadata": {},
   "source": [
    "ViT 整體架構可以透過以下五個步驟描述：\n",
    "\n",
    "1. 將輸入影像切分為多個 patches\n",
    "將影像分割成小的區塊（patches），作為後續處理的基礎單位。\n",
    "\n",
    "2. 從每個 patch 中獲取線性嵌入（表徵，representation），稱為 Patch Embeddings\n",
    "使用線性投影對每個 patch 進行特徵提取，轉換為一維向量表示。\n",
    "\n",
    "3. 將位置嵌入（Position Embeddings）和 `[cls] token` 添加到每個 Patch Embeddings\n",
    "為每個 patch 加入位置資訊以保留空間結構，並插入一個特別的 `[cls] token`，用於分類任務的輸出。\n",
    "\n",
    "4. 通過 Transformer Encoder 並獲取 [cls] token 的輸出值\n",
    "將所有嵌入（包含 `[cls] token`）輸入 Transformer Encoder，進行多層處理，最後提取 `[cls] token` 的輸出。\n",
    "\n",
    "5. 通過 `MLP Head` 傳遞 `[cls] token` 的表徵以獲得最終的類別預測\n",
    "將 `[cls] token` 的最終表徵傳入 `MLP Head`，得到分類結果。\n",
    "\n",
    "注意：\n",
    "Transformer Encoder 內部包含一個 `MLP`，但它與用於分類的 `MLP Head` 是不同的結構。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0944a61f",
   "metadata": {},
   "source": [
    "<img src=\"./images/vit-01.png\" width=\"1000\" align=\"left\" alt=\"\" title=\"fig-2 Simplified Model Overview\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e06ef1c",
   "metadata": {},
   "source": [
    "現在具體看一下這五個步驟。 假設要對大小為 `224 x 224` 的青蛙的 `3` 通道 (RGB) 輸入圖像進行分類。\n",
    "\n",
    " <strong>第一步</strong> 是在影像上創建大小為 `16 x 16` 的patches。因此創建 `14 x 14` 或 `196` 個這樣的 patches。 可以將這些 patches 放在一條直線上，其中第一個 patch 來自輸入影像的左上角，最後一個 patch 來自右下角。從圖中可以看出，patch 大小為 `3 x 16 x 16 = 768`，其中 `3`表示通道數（RGB）。\n",
    "\n",
    "在 <strong>第二步</strong>中，將這些 patches 通過線性投影層，獲得每個影像 patch 的 `1 x 768` 矢量表示（`3 x 16 x 16 = 768`），這些表示在圖中以紫色顯示。在論文中，作者將 patches 的這些表徵稱為 Patch Embeddings。因為總共有 `196` 個 patches，每個 patch 都表示為一個 `1 x 768` 長的向量。因此，patch embedding matrix 的總大小為 `196 x 768`。\n",
    "\n",
    "在<strong>第三步</strong>中，採用大小為 `196 x 768` 且類似於 BERT 的 patch embedding matrix，將 `[cls]` token前置添加到該 embedded patches 序列中。添加 `[cls]` token後，Patch Embeddings 的大小變為 `197 x 768`，然後與 Position Embeddings 相加，Position Embeddings 的大小也為 `197 x 768`。\n",
    "\n",
    "在<strong>第四步</strong>中，將這些帶有位置信息和前置 `[cls]` token的預處理 patch embeddings 傳遞給 Transformer 編碼器，並獲得 `[cls]` token的學習特徵。Transformer Encoder 的輸出的大小為 `1 x 768`，然後作為最後<strong>第五步</strong>的一部分被餵送到 `MLP Head`（線性層）以獲得類別預測。\n",
    "\n",
    "在查看了整體架構之後，現在將在以下部分中詳細查看各個步驟。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a2a350",
   "metadata": {},
   "source": [
    "## Patch Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0e0bc",
   "metadata": {},
   "source": [
    "<p>In this section we will be looking at <strong>steps one and two</strong> in detail. That is the process of <u>getting patch embeddings from an input image</u>.</p>\n",
    "\n",
    "<img src=\"./images/vit-02.png\" width=\"1000\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572aeb2",
   "metadata": {},
   "source": [
    "到目前為止，從一個輸入影像中獲取 patch embeddings 的方式是首先將影像分割成固定大小的 patches，然後使用線性投影層獲得每個 patch 的線性嵌入。\n",
    "\n",
    "但是，實際上可以使用<strong>2D 卷積</strong>操作將這兩個步驟合併成一個步驟。從實現的角度來看，這樣做更好，因為 GPU 已針對執行卷積操作進行了優化，並且無需先將圖像拆分為區塊。\n",
    "\n",
    "如果將 `out_channels` 的數量設置為 `768`，並且將 `kernel_size` 和 `stride` 都設置為 `16`，那麼如圖所示，一旦執行卷積操作（其中 2-D Convolution 的核大小為 `3 x 16 x 16`），可以得到大小為 `196 x 768` 的<strong>Patch Embeddings</strong> matrix，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951274d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input image `B, C, H, W`\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "# 2D conv\n",
    "conv = nn.Conv2d(3, 768, 16, 16)\n",
    "conv(x).reshape(-1, 196).transpose(0,1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b52a594",
   "metadata": {},
   "source": [
    "## [cls] token & Position Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99762614",
   "metadata": {},
   "source": [
    "在<strong>第三步</strong>，前置添加 [cls] token，並將 Positional Embeddings 相加到 Patch Embeddings 中。\n",
    "\n",
    "與 BERT 的 [class] token 類似，將可學習的嵌入添加到 Patch Embedding 序列中，其在 Transformer 編碼器（稱為<strong>Z<sub>L</sub><sup>0</sup></strong>）輸出處的狀態用作影像特徵。在預訓練和微調期間，分類頭都附加到<strong>Z<sub>L</sub><sup>0</sup></strong>。\n",
    "\n",
    "Position Embeddings 也被添加到 Patch Embeddings 中以保留位置信息。使用標準的可學習一維位置嵌入，生成的嵌入向量序列用作編碼器的輸入。\n",
    "\n",
    "這個過程可以很容易地可視化如下：\n",
    "\n",
    "<img src=\"./images/vit-03.png\" width=\"1000\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11cded",
   "metadata": {},
   "source": [
    "<p>As can be seen , the <code class=\"language-plaintext highlighter-rouge\">[cls]</code> token is a vector of size <code class=\"language-plaintext highlighter-rouge\">1 x 768</code>. We <strong>prepend</strong> it to the <strong>Patch Embeddings</strong>, thus, the updated size of <strong>Patch Embeddings</strong> becomes <code class=\"language-plaintext highlighter-rouge\">197 x 768</code>.</p>\n",
    "\n",
    "<p>Next, we add <strong>Positional Embeddings</strong> of size <code class=\"language-plaintext highlighter-rouge\">197 x 768</code> to the <strong>Patch Embeddings</strong> with <code class=\"language-plaintext highlighter-rouge\">[cls]</code> token to get <strong>combined embeddings</strong> which are then fed to the <code class=\"language-plaintext highlighter-rouge\">Transformer Encoder</code>. This is a pretty standard step that comes from the original Transformer paper - <a href=\"https://arxiv.org/abs/1706.03762\">Attention is all you need</a>.</p>\n",
    "\n",
    "<blockquote>\n",
    "  <p>Note that the Positional Embeddings and <code class=\"language-plaintext highlighter-rouge\">cls</code> token vector is nothing fancy but rather just a trainable <code class=\"language-plaintext highlighter-rouge\">nn.Parameter</code> matrix/vector.</p>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae0501",
   "metadata": {},
   "source": [
    "##  Transformer Encoder\n",
    "\n",
    "\n",
    "在本節中，我們將詳細研究 <strong>Transformer Encoder</strong>。 <strong>Transformer 編碼器</strong> 由<strong>多頭注意力（Multi-Head Attention）</strong> 和 <strong>MLP</strong> 塊的交替組成。此外，在每個塊之前使用 Layer Norm，在每個塊之後使用殘差連接。\n",
    "\n",
    "<strong>Transformer Encoder</strong> 的層/塊（layer/block）可以如下所示進行可視化：\n",
    "\n",
    "<img src=\"./images/vit-07.png\"  width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d5038",
   "metadata": {},
   "source": [
    "<p>The first layer of the <strong>Transformer Encoder</strong> accepts <strong>combined embeddings</strong> of shape<code class=\"language-plaintext highlighter-rouge\">197 x 768</code> as input. For all subsequent layers, the inputs are the outputs <code class=\"language-plaintext highlighter-rouge\">Out</code> matrix of shape <code class=\"language-plaintext highlighter-rouge\">197 x 768</code> from the previous layer of the <strong>Transformer Encoder</strong>. There are a total of <u>12 such layers</u> in the <strong>Transformer Encoder</strong> of the ViT-Base architecture.</p>\n",
    "\n",
    "<p>Inside the layer, the inputs are first passed through a <strong>Layer Norm</strong>, and then fed to <strong>Multi-Head Attention</strong> block.</p>\n",
    "\n",
    "<p>Inside the <strong>Multi-Head Attention</strong>, the inputs are first converted to <code class=\"language-plaintext highlighter-rouge\">197 x 2304 (768*3)</code> shape using a <strong>Linear layer</strong> to get the <strong>qkv</strong> matrix. Next we reshape this <strong>qkv</strong> matrix into <code class=\"language-plaintext highlighter-rouge\">197 x 3 x 768</code> where each of the three matrices of shape <code class=\"language-plaintext highlighter-rouge\">197 x 768</code> represent the <strong>q</strong>, <strong>k</strong> and <strong>v</strong> matrices. These <strong>q</strong>, <strong>k</strong> and <strong>v</strong> matrices are further reshaped to <code class=\"language-plaintext highlighter-rouge\">12 x 197 x 64</code> to represent the 12 attention heads. Once we have the <strong>q</strong>, <strong>k</strong> and <strong>v</strong> matrices, we finally perform the attention operation inside the <strong>Multi-Head Attention</strong> block which is given by the equation:</p>\n",
    "\n",
    "<img src=\"./images/vit-08.png\" alt=\"\" width=\"1000\" />\n",
    "\n",
    "<p>Once we get the outputs from the <strong>Multi-Head Attention</strong> block, these are added to the inputs (skip connection) to get the final outouts that again get passed to <strong>Layer Norm</strong> before being fed to the <strong>MLP</strong> Block.</p>\n",
    "\n",
    "<p>The <strong>MLP</strong>, is a Multi-Layer Perceptron block consists of two linear layers and a GELU non-linearity. The outputs from the <strong>MLP</strong> block are again added to the inputs (skip connection) to get the final output from one layer of the <strong>Transformer Encoder</strong>.</p>\n",
    "\n",
    "<p>Having looked at a single layer inside the <strong>Transformer Encoder</strong>, let’s now zoom out and look at the complete <strong>Transformer Encoder</strong>.</p>\n",
    "\n",
    "<img src=\"./images/vit-06.png\" alt=\"\" title=\"fig-6 Transformer Encoder\" />\n",
    "    \n",
    "從上圖可以看出，一個 <strong>Transformer Encoder</strong> 由 12 層組成。第一層的輸出被餵送到第二層，第二層的輸出被餵送到第三層，直到我們從 <strong>Transformer Encoder</strong> 的第 12 層獲得最終輸出，然後將其餵送到 <strong>MLP Head</strong> 以獲得類別預測。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceeeda5",
   "metadata": {},
   "source": [
    "## The Vision Transformer in PyTorch \n",
    "\n",
    "在詳細了解了 Vision Transformer 架構之後，現在讓我們看一下如何在 PyTorch 中實現該架構。我們將參考 <a href=\"https://github.com/rwightman/pytorch-image-models\">timm</a> 的程式碼來解釋實現過程。下面的程式碼段直接從 <a href=\"https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\">here</a> 複製而來。\n",
    "\n",
    "首先，我們從底層開始構建 Vision Transformer。那麼，如何獲得 Patch Embeddings 呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a8ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909695ac",
   "metadata": {},
   "source": [
    "<p>As we know, we use a <strong>2-D Convolution</strong> where <code class=\"language-plaintext highlighter-rouge\">stride</code>, <code class=\"language-plaintext highlighter-rouge\">kernel_size</code> are set to <code class=\"language-plaintext highlighter-rouge\">patch_size</code>. Thus, that is exactly what the class above does. We set <code class=\"language-plaintext highlighter-rouge\">self.proj</code> to be a <code class=\"language-plaintext highlighter-rouge\">nn.Conv2d</code> which goes from 3-channels to <code class=\"language-plaintext highlighter-rouge\">768</code> and to get <code class=\"language-plaintext highlighter-rouge\">196 x 768</code> patch embedding matrix.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55e0e5",
   "metadata": {},
   "source": [
    "```shell\n",
    "patch_embed = PatchEmbed()\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "patch_embed(x).shape \n",
    ">> torch.Size([1, 196, 768])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89873e67",
   "metadata": {},
   "source": [
    "It is also pretty easy to implement the MLP Block inside the Transformer Encoder below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d66eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89bff52",
   "metadata": {},
   "source": [
    "Basically, it consists of two layers and a GELU activation layer. There isn’t a lot happening in this class and is pretty easy to implement. Next, we implement Attention as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8dc2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10960ac",
   "metadata": {},
   "source": [
    "<p>As described inside the <strong>Multi-Head Attention</strong> block, we use a Linear layer to get the <strong>qkv</strong> matrix. Also, we apply the attention operation inside the <code class=\"language-plaintext highlighter-rouge\">forward</code> method above like so:</p>\n",
    "\n",
    "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>        <span class=\"n\">attn</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">q</span> <span class=\"o\">@</span> <span class=\"n\">k</span><span class=\"p\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">scale</span>\n",
    "        <span class=\"n\">attn</span> <span class=\"o\">=</span> <span class=\"n\">attn</span><span class=\"p\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
    "</code></pre></div></div>\n",
    "<p>The above code implements attention equation. Since we have already implemented the <strong>Attention</strong> Layer and <strong>MLP</strong> block, let’s quickly implement a single layer of the <strong>Transformer Encoder</strong>. A single <code class=\"language-plaintext highlighter-rouge\">Block</code> consists of Layer Norm, Attention and MLP block.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222d849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4936796",
   "metadata": {},
   "source": [
    "從上面的 `forward` 方法可以看出，這個 `Block` 接受輸入 `x`，將它們傳遞給 `self.norm1`，這是 `LayerNorm`，然後進行注意力操作。接下來，在通過 `self.mlp` 和 `Dropout` 之前，再次對注意力操作後的輸出進行歸一化，以從該單個 block 中獲得輸出 `Out` 矩陣。\n",
    "\n",
    "現在我們已經有了所有的部分，Vision Transformer 的完整架構可以如下實現："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4b436da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  \n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681deece",
   "metadata": {},
   "source": [
    "## timm 官方程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a704f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
    "        - https://arxiv.org/abs/2010.11929\n",
    "    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`\n",
    "        - https://arxiv.org/abs/2012.12877\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None, weight_init=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "            weight_init: (str): weight init scheme\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.init_weights(weight_init)\n",
    "\n",
    "    def init_weights(self, mode=''):\n",
    "        assert mode in ('jax', 'jax_nlhb', 'nlhb', '')\n",
    "        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        if self.dist_token is not None:\n",
    "            trunc_normal_(self.dist_token, std=.02)\n",
    "        if mode.startswith('jax'):\n",
    "            # leave cls token as zeros to match jax impl\n",
    "            named_apply(partial(_init_vit_weights, head_bias=head_bias, jax_impl=True), self)\n",
    "        else:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "            self.apply(_init_vit_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # this fn left here for compat with downstream users\n",
    "        _init_vit_weights(m)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=''):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        if self.dist_token is None:\n",
    "            return self.head\n",
    "        else:\n",
    "            return self.head, self.head_dist\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        if self.num_tokens == 2:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None:\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\n",
    "            if self.training and not torch.jit.is_scripting():\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else:\n",
    "                return (x + x_dist) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f2eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypytorch",
   "language": "python",
   "name": "mypytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
